{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tcn import TCN\n",
    "import os\n",
    "\n",
    "os.chdir(\"../src/\")\n",
    "from models import tcn\n",
    "from utils import auxiliary_plots, metrics\n",
    "from preprocessing import normalization, data_generation\n",
    "\n",
    "SEED = 1\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "os.chdir(\"../notebooks/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "TRAIN_FILE_NAME = '../data/hourly_20140102_20191101_train.csv'\n",
    "TEST_FILE_NAME = '../data/hourly_20140102_20191101_test.csv'\n",
    "\n",
    "FORECAST_HORIZON = 24\n",
    "PAST_HISTORY = 192\n",
    "BATCH_SIZE = 256\n",
    "BUFFER_SIZE = 10000\n",
    "EPOCHS = 25\n",
    "METRICS = ['mape']\n",
    "\n",
    "TCN_PARAMS = {\n",
    "    'nb_filters': 128,\n",
    "    'kernel_size': 3,\n",
    "    'nb_stacks': 1,\n",
    "    'dilations': [1, 2, 4, 8, 16, 32, 64],\n",
    "    'dropout_rate': 0,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Read train file\n",
    "with open(TRAIN_FILE_NAME, 'r') as datafile:\n",
    "    ts_train = datafile.readlines()[1:]  # skip the header\n",
    "    ts_train = np.asarray([np.asarray(l.rstrip().split(',')[0], dtype=np.float32) for l in ts_train])\n",
    "    ts_train = np.reshape(ts_train, (ts_train.shape[0],))\n",
    "\n",
    "# Read test data file\n",
    "with open(TEST_FILE_NAME, 'r') as datafile:\n",
    "    ts_test = datafile.readlines()[1:]  # skip the header\n",
    "    ts_test = np.asarray([np.asarray(l.rstrip().split(',')[0], dtype=np.float32) for l in ts_test])\n",
    "    ts_test = np.reshape(ts_test, (ts_test.shape[0],))\n",
    "    \n",
    "# Train/validation split\n",
    "TRAIN_SPLIT = int(ts_train.shape[0] * 0.8)\n",
    "\n",
    "# Normalize training data\n",
    "norm_params = normalization.get_normalization_params(ts_train[:TRAIN_SPLIT])\n",
    "ts_train = normalization.normalize(ts_train, norm_params)\n",
    "# Normalize test data with train params\n",
    "ts_test = normalization.normalize(ts_test, norm_params)\n",
    "\n",
    "# Get x and y for training and validation\n",
    "x_train, y_train = data_generation.univariate_data(ts_train, 0, TRAIN_SPLIT, PAST_HISTORY, FORECAST_HORIZON)\n",
    "x_val, y_val = data_generation.univariate_data(ts_train, TRAIN_SPLIT - PAST_HISTORY, ts_train.shape[0],\n",
    "                                                   PAST_HISTORY, FORECAST_HORIZON)\n",
    "\n",
    "# Get x and y for test data\n",
    "x_test, y_test = data_generation.univariate_data(ts_test, 0, ts_test.shape[0], PAST_HISTORY, FORECAST_HORIZON)\n",
    "\n",
    "# Convert numpy data to tensorflow dataset\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train)).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "val_data = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(BATCH_SIZE).repeat()\n",
    "test_data = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TCN: Create and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "model = tcn(x_train.shape, FORECAST_HORIZON, 'adam', 'mae', \n",
    "            nb_filters=TCN_PARAMS['nb_filters'],\n",
    "            kernel_size=TCN_PARAMS['kernel_size'],\n",
    "            nb_stacks= TCN_PARAMS['nb_stacks'],\n",
    "            dilations=TCN_PARAMS['dilations'],\n",
    "            dropout_rate=TCN_PARAMS['dropout_rate'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "TRAIN_MODEL = True   \n",
    "\n",
    "checkpoint_path = \"training_tcn/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "if TRAIN_MODEL:\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                     save_weights_only=True,\n",
    "                                                     verbose=1)\n",
    "    evaluation_interval = int(np.ceil(x_train.shape[0] / BATCH_SIZE))\n",
    "    history = model.fit(train_data, \n",
    "                        epochs=EPOCHS,\n",
    "                        steps_per_epoch=evaluation_interval,\n",
    "                        validation_data=val_data, validation_steps=evaluation_interval,\n",
    "                        callbacks=[cp_callback])\n",
    "    auxiliary_plots.plot_training_history(history, ['loss'])\n",
    "\n",
    "model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "val_forecast = model.predict(x_val)\n",
    "val_forecast = normalization.denormalize(val_forecast, norm_params)\n",
    "y_val_denormalized = normalization.denormalize(y_val, norm_params)\n",
    "val_metrics = metrics.evaluate(y_val_denormalized, val_forecast, METRICS)\n",
    "print('Validation scores', val_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_forecast = model.predict(test_data)\n",
    "test_forecast = normalization.denormalize(test_forecast, norm_params)\n",
    "y_test_denormalized = normalization.denormalize(y_test, norm_params)\n",
    "x_test_denormalized = normalization.denormalize(x_test, norm_params)\n",
    "test_metrics = metrics.evaluate(y_test_denormalized, test_forecast, METRICS)\n",
    "print('Test scores', test_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
