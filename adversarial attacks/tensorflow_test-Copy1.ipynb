{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "695ceb3d-a2ee-42a2-8ac0-a9f4d3efc64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-15 12:23:14.690900: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-15 12:23:14.690933: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-15 12:23:14.690959: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-15 12:23:14.697741: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from src.models import lstm, tcn\n",
    "from src.utils import auxiliary_plots, metrics\n",
    "from src.utils.print_functions import notify_slack\n",
    "from src.preprocessing import normalization, data_generation\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b382909f-ff33-4b9d-a204-1d7706c62c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from src.experiments import run_experiments\n",
    "import os\n",
    "\n",
    "WEBHOOK = os.environ.get('webhook_slack')\n",
    "print(WEBHOOK)\n",
    "\n",
    "METRICS = ['mse', 'rmse', 'nrmse', 'mae', 'wape', 'mpe', 'mape', 'mdape', 'smape', 'smdape',\n",
    "           'mase', 'rmspe', 'rmsse', 'mre', 'rae', 'mrae', 'std_ae', 'std_ape']\n",
    "\n",
    "TCN_PARAMS = {\n",
    "    'nb_filters': [32, 64, 128],\n",
    "    'kernel_size': [2, 3, 4, 5, 6],\n",
    "    'nb_stacks': [1, 2, 3, 4, 5],\n",
    "    'dilations': [[1, 2, 4], [1, 2, 4, 8], [1, 2, 4, 8, 16], [1, 2, 4, 8, 16, 32], [1, 2, 4, 8, 16, 32, 64],\n",
    "                  [1, 3, 9], [1, 3, 9, 27], [1, 3, 9, 27, 81],\n",
    "                  [1, 3, 6], [1, 3, 6, 12], [1, 3, 6, 12, 24], [1, 3, 6, 12, 24], [1, 3, 6, 12, 24, 48],\n",
    "                  [1, 5, 7], [1, 5, 7, 14], [1, 5, 7, 14, 28], [1, 5, 7, 14, 28, 56]],\n",
    "    'dropout_rate': [0],\n",
    "}\n",
    "\n",
    "TCN_PARAMS = {\n",
    "    # 'nb_filters': [32, 64, 128],\n",
    "    # 'kernel_size': [2, 3, 4, 5],\n",
    "    # 'nb_stacks': [1, 2, 3, 4],\n",
    "    # 'dilations': [[1, 2, 4], [1, 2, 4, 8], [1, 2, 4, 8, 16], [1, 2, 4, 8, 16, 32], [1, 2, 4, 8, 16, 32, 64],\n",
    "    #               [1, 3, 6], [1, 3, 6, 12], [1, 5, 7], [1, 5, 7, 14]],\n",
    "    # 'dropout_rate': [0],\n",
    "    'nb_filters': [128],\n",
    "    'kernel_size': [6],\n",
    "    'nb_stacks': [1, 2, 3, 4],\n",
    "    'dilations': [[1, 3, 6, 12]],\n",
    "    'dropout_rate': [0],\n",
    "}\n",
    "LSTM_PARAMS = {\n",
    "    'num_stack_layers': [1, 2, 3],\n",
    "    'units': [32, 64, 128],\n",
    "    'dropout': [0]\n",
    "}\n",
    "\n",
    "FORECAST_HORIZON = 24\n",
    "PAST_HISTORY = [144]\n",
    "\n",
    "BATCH_SIZE = [128]\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "EPOCHS = [50]\n",
    "\n",
    "_GPU_NUMBER = None\n",
    "\n",
    "# Electric demand forecasting\n",
    "#run_experiments(train_file_name='data/hourly_20140102_20191101_train.csv',\n",
    "#                test_file_name='data/hourly_20140102_20191101_test.csv',\n",
    "#                result_file_name='experimental_results_electricity.csv',\n",
    "#                forecast_horizon=FORECAST_HORIZON,\n",
    "#                past_history_ls=PAST_HISTORY,\n",
    "#                batch_size_ls=BATCH_SIZE,\n",
    "#                epochs_ls=EPOCHS,\n",
    "#                tcn_params=TCN_PARAMS,\n",
    "#                lstm_params=None,\n",
    "#                gpu_number=_GPU_NUMBER,\n",
    "#                metrics_ls=METRICS,\n",
    "#                buffer_size=1000,\n",
    "#                seed=1,\n",
    "#                show_plots=False,\n",
    "#                webhook=WEBHOOK,\n",
    "#                validation_size=0.)\n",
    "\n",
    "# Electric vehicle power consumption forecasting\n",
    "# run_experiments(train_file_name='data/CECOVEL_train.csv',\n",
    "#                 test_file_name='data/CECOVEL_test.csv',\n",
    "#                 result_file_name='files/results/experimental_results_EV.csv',\n",
    "#                 forecast_horizon=FORECAST_HORIZON,\n",
    "#                 past_history_ls=PAST_HISTORY,\n",
    "#                 batch_size_ls=BATCH_SIZE,\n",
    "#                 epochs_ls=EPOCHS,\n",
    "#                 tcn_params=TCN_PARAMS,\n",
    "#                 lstm_params=LSTM_PARAMS,\n",
    "#                 gpu_number=None,\n",
    "#                 metrics_ls=METRICS,\n",
    "#                 buffer_size=1000,\n",
    "#                 seed=1,\n",
    "#                 show_plots=False,\n",
    "#                 webhook=WEBHOOK,\n",
    "#                 validation_size=0.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe958679-af15-4189-ae5d-b0a3d99a1819",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "result_file_name= 'experimental_results_electricity.csv'\n",
    "metrics_ls=METRICS\n",
    "train_file_name='data/hourly_20140102_20191101_train.csv'\n",
    "test_file_name='data/hourly_20140102_20191101_test.csv'\n",
    "result_file_name='experimental_results_electricity.csv'\n",
    "forecast_horizon=FORECAST_HORIZON\n",
    "past_history_ls=PAST_HISTORY\n",
    "batch_size_ls=BATCH_SIZE\n",
    "epochs_ls=EPOCHS\n",
    "tcn_params=TCN_PARAMS\n",
    "lstm_params=None\n",
    "gpu_number=_GPU_NUMBER\n",
    "metrics_ls=METRICS\n",
    "buffer_size=1000\n",
    "seed=1\n",
    "show_plots=False\n",
    "webhook=WEBHOOK\n",
    "validation_size=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d49df411-9e46-4f01-84fd-00262088f761",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(train_file_name, test_file_name, result_file_name, forecast_horizon, past_history_ls, batch_size_ls,\n",
    "#epochs_ls, tcn_params=TCN_PARAMS, lstm_params=LSTM_PARAMS, gpu_number=None, metrics_ls=METRICS,\n",
    "#buffer_size=1000, seed=1, show_plots=False, webhook=None, validation_size=0.2)\n",
    "\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81d83a17-c1a7-4c9b-b36f-b7fa8fa4344e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "device_name = str(gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64e09860-408d-4ed5-86e7-b18347aa161a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CURRENT INDEX -1\n"
     ]
    }
   ],
   "source": [
    "# Write result csv header\n",
    "current_index = 0\n",
    "try:\n",
    "    with open(result_file_name, 'r') as resfile:\n",
    "        current_index = sum(1 for line in resfile) - 1\n",
    "except IOError:\n",
    "    pass\n",
    "print('CURRENT INDEX', current_index)\n",
    "if current_index == 0:\n",
    "    with open(result_file_name, 'w') as resfile:\n",
    "        resfile.write(';'.join([str(a) for a in\n",
    "                                ['MODEL', 'MODEL_DESCRIPTION', 'FORECAST_HORIZON', 'PAST_HISTORY', 'BATCH_SIZE',\n",
    "                                 'EPOCHS'] +\n",
    "                                metrics_ls + ['val_' + m for m in metrics_ls] + ['loss', 'val_loss',\n",
    "                                                                                 'Execution_time',\n",
    "                                                                                 'Device']]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad87ae1b-256b-4398-a6dc-94a2f9c78308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read train file\n",
    "with open(train_file_name, 'r') as datafile:\n",
    "    ts_train = datafile.readlines()[1:]  # skip the header\n",
    "    ts_train = np.asarray([np.asarray(l.rstrip().split(',')[0], dtype=np.float32) for l in ts_train])\n",
    "    ts_train = np.reshape(ts_train, (ts_train.shape[0],))\n",
    "\n",
    "# Read test data file\n",
    "with open(test_file_name, 'r') as datafile:\n",
    "    ts_test = datafile.readlines()[1:]  # skip the header\n",
    "    ts_test = np.asarray([np.asarray(l.rstrip().split(',')[0], dtype=np.float32) for l in ts_test])\n",
    "    ts_test = np.reshape(ts_test, (ts_test.shape[0],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0543c4f-a26d-4f29-b1f7-578099bd194f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(245376,) 220838\n"
     ]
    }
   ],
   "source": [
    "# Train/validation split\n",
    "TRAIN_SPLIT = int(ts_train.shape[0] * (1 - validation_size))\n",
    "print(ts_train.shape, TRAIN_SPLIT)\n",
    "# Normalize training data\n",
    "norm_params = normalization.get_normalization_params(ts_train[:TRAIN_SPLIT])\n",
    "ts_train = normalization.normalize(ts_train, norm_params)\n",
    "# Normalize test data with train params\n",
    "ts_test = normalization.normalize(ts_test, norm_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44f562dd-33c7-4161-be65-13dfa9c543cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "index_1, total_1 = 0, len(list(itertools.product(past_history_ls, batch_size_ls, epochs_ls)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f49f6a09-e546-444c-b6c8-7e0c36907746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(144, 128, 50)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(itertools.product(past_history_ls, batch_size_ls, epochs_ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42fc603b-612b-49c5-8e79-bf71ca9cebe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                             | 0/1 [00:00<?, ?it/s]\n",
      "  0%|                                                                             | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 144, 1)]          0         \n",
      "                                                                 \n",
      " tcn (TCN)                   (None, 128)               1477632   \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 24)                3096      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 24)                600       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1481328 (5.65 MB)\n",
      "Trainable params: 1481328 (5.65 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.66it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for past_history, batch_size, epochs in tqdm(list(itertools.product(past_history_ls, batch_size_ls, epochs_ls))):\n",
    "    index_1 += 1\n",
    "    # Get x and y for training and validation\n",
    "    #x_train, y_train = data_generation.univariate_data(ts_train, 0, TRAIN_SPLIT, past_history, forecast_horizon)\n",
    "    #x_val, y_val = data_generation.univariate_data(ts_train, TRAIN_SPLIT - past_history, ts_train.shape[0],\n",
    "    #                                               past_history, forecast_horizon)\n",
    "    #print(x_train.shape, y_train.shape, '\\n', x_val.shape, y_val.shape)\n",
    "    # Get x and y for test data\n",
    "    #x_test, y_test = data_generation.univariate_data(ts_test, 0, ts_test.shape[0], past_history, forecast_horizon)\n",
    "\n",
    "    # Convert numpy data to tensorflow dataset\n",
    "    #train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train)).cache().shuffle(buffer_size).batch(\n",
    "    #    batch_size).repeat()\n",
    "    #val_data = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(\n",
    "    #    batch_size).repeat() if validation_size > 0 else None\n",
    "    #test_data = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
    "    \n",
    "    save_best = tf.keras.callbacks.ModelCheckpoint(filepath=f'best-model-bs-{batch_size}-ph-{past_history}.ckpt',save_best_only=True, monitor='val_loss')\n",
    "    \n",
    "    # Create models\n",
    "    model_list = {}\n",
    "    model_description_list = {}\n",
    "    if tcn_params is not None:\n",
    "        model_list = {'TCN_{}'.format(j): (tcn, [x_train.shape, forecast_horizon, 'adam', 'mae', *params]) for\n",
    "                      j, params in\n",
    "                      enumerate(itertools.product(*tcn_params.values())) if\n",
    "                      params[1] * params[2] * params[3][-1] == past_history}\n",
    "        model_description_list = {'TCN_{}'.format(j): str(dict(zip(tcn_params.keys(), params))) for j, params in\n",
    "                                  enumerate(itertools.product(*tcn_params.values())) if\n",
    "                                  params[1] * params[2] * params[3][-1] == past_history}\n",
    "    if lstm_params is not None:\n",
    "        model_list = {**model_list,\n",
    "                      **{'LSTM_{}'.format(j): (lstm, [x_train.shape, forecast_horizon, 'adam', 'mae', *params]) for\n",
    "                         j, params in\n",
    "                         enumerate(itertools.product(*lstm_params.values()))}}\n",
    "        model_description_list = {**model_description_list,\n",
    "                                  **{'LSTM_{}'.format(j): str(dict(zip(lstm_params.keys(), params))) for j, params\n",
    "                                     in\n",
    "                                     enumerate(itertools.product(*lstm_params.values()))}}\n",
    "\n",
    "    steps_per_epoch = int(np.ceil(x_train.shape[0] / batch_size))\n",
    "    validation_steps = None\n",
    "\n",
    "    index_2, total_2 = 0, len(model_list.keys())\n",
    "    for model_name, (model_function, params) in tqdm(model_list.items(), position=1):\n",
    "        index_2 += 1\n",
    "        i += 1\n",
    "        if i <= current_index:\n",
    "            continue\n",
    "        start = time.time()\n",
    "        model = model_function(*params)\n",
    "        print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "688257fd-fd14-4575-a8d8-7bc8b3687b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function src.models.tcn(input_shape, output_size=1, optimizer='adam', loss='mae', nb_filters=64, kernel_size=2, nb_stacks=1, dilations=[1, 2, 4, 8, 16, 32], dropout_rate=0, use_skip_connections=True, use_batch_norm=False, activation='linear', return_sequences=False, dense_layers=[], dense_dropout=0.0)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "406c03da-09dc-4899-a0a6-e335c8648bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(220671, 144, 1), 24, 'adam', 'mae', 128, 6, 2, [1, 3, 6, 12], 0]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ec5b7b9-b20c-4810-912f-40a06bcb7a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function src.models.tcn(input_shape, output_size=1, optimizer='adam', loss='mae', nb_filters=64, kernel_size=2, nb_stacks=1, dilations=[1, 2, 4, 8, 16, 32], dropout_rate=0, use_skip_connections=True, use_batch_norm=False, activation='linear', return_sequences=False, dense_layers=[], dense_dropout=0.0)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3a8478-6146-46c7-8b02-ac310a11a047",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
