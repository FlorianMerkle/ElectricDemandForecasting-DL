{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019b9f1f-a43d-4f35-9153-a535ad0924ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "695ceb3d-a2ee-42a2-8ac0-a9f4d3efc64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from src.models import lstm, tcn\n",
    "from src.utils import auxiliary_plots, metrics\n",
    "from src.utils.print_functions import notify_slack\n",
    "from src.preprocessing import normalization, data_generation\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b382909f-ff33-4b9d-a204-1d7706c62c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from src.experiments import run_experiments\n",
    "import os\n",
    "\n",
    "WEBHOOK = os.environ.get('webhook_slack')\n",
    "print(WEBHOOK)\n",
    "\n",
    "METRICS = ['mse', 'rmse', 'nrmse', 'mae', 'wape', 'mpe', 'mape', 'mdape', 'smape', 'smdape',\n",
    "           'mase', 'rmspe', 'rmsse', 'mre', 'rae', 'mrae', 'std_ae', 'std_ape']\n",
    "\n",
    "TCN_PARAMS = {\n",
    "    'nb_filters': [32, 64, 128],\n",
    "    'kernel_size': [2, 3, 4, 5, 6],\n",
    "    'nb_stacks': [1, 2, 3, 4, 5],\n",
    "    'dilations': [[1, 2, 4], [1, 2, 4, 8], [1, 2, 4, 8, 16], [1, 2, 4, 8, 16, 32], [1, 2, 4, 8, 16, 32, 64],\n",
    "                  [1, 3, 9], [1, 3, 9, 27], [1, 3, 9, 27, 81],\n",
    "                  [1, 3, 6], [1, 3, 6, 12], [1, 3, 6, 12, 24], [1, 3, 6, 12, 24], [1, 3, 6, 12, 24, 48],\n",
    "                  [1, 5, 7], [1, 5, 7, 14], [1, 5, 7, 14, 28], [1, 5, 7, 14, 28, 56]],\n",
    "    'dropout_rate': [0],\n",
    "}\n",
    "\n",
    "TCN_PARAMS = {\n",
    "    # 'nb_filters': [32, 64, 128],\n",
    "    # 'kernel_size': [2, 3, 4, 5],\n",
    "    # 'nb_stacks': [1, 2, 3, 4],\n",
    "    # 'dilations': [[1, 2, 4], [1, 2, 4, 8], [1, 2, 4, 8, 16], [1, 2, 4, 8, 16, 32], [1, 2, 4, 8, 16, 32, 64],\n",
    "    #               [1, 3, 6], [1, 3, 6, 12], [1, 5, 7], [1, 5, 7, 14]],\n",
    "    # 'dropout_rate': [0],\n",
    "    'nb_filters': [128],\n",
    "    'kernel_size': [6],\n",
    "    'nb_stacks': [1, 2, 3, 4],\n",
    "    'dilations': [[1, 3, 6, 12]],\n",
    "    'dropout_rate': [0],\n",
    "}\n",
    "LSTM_PARAMS = {\n",
    "    'num_stack_layers': [1, 2, 3],\n",
    "    'units': [32, 64, 128],\n",
    "    'dropout': [0]\n",
    "}\n",
    "\n",
    "FORECAST_HORIZON = 24\n",
    "PAST_HISTORY = [144]\n",
    "\n",
    "BATCH_SIZE = [128]\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "EPOCHS = [50]\n",
    "\n",
    "_GPU_NUMBER = None\n",
    "\n",
    "# Electric demand forecasting\n",
    "#run_experiments(train_file_name='data/hourly_20140102_20191101_train.csv',\n",
    "#                test_file_name='data/hourly_20140102_20191101_test.csv',\n",
    "#                result_file_name='experimental_results_electricity.csv',\n",
    "#                forecast_horizon=FORECAST_HORIZON,\n",
    "#                past_history_ls=PAST_HISTORY,\n",
    "#                batch_size_ls=BATCH_SIZE,\n",
    "#                epochs_ls=EPOCHS,\n",
    "#                tcn_params=TCN_PARAMS,\n",
    "#                lstm_params=None,\n",
    "#                gpu_number=_GPU_NUMBER,\n",
    "#                metrics_ls=METRICS,\n",
    "#                buffer_size=1000,\n",
    "#                seed=1,\n",
    "#                show_plots=False,\n",
    "#                webhook=WEBHOOK,\n",
    "#                validation_size=0.)\n",
    "\n",
    "# Electric vehicle power consumption forecasting\n",
    "# run_experiments(train_file_name='data/CECOVEL_train.csv',\n",
    "#                 test_file_name='data/CECOVEL_test.csv',\n",
    "#                 result_file_name='files/results/experimental_results_EV.csv',\n",
    "#                 forecast_horizon=FORECAST_HORIZON,\n",
    "#                 past_history_ls=PAST_HISTORY,\n",
    "#                 batch_size_ls=BATCH_SIZE,\n",
    "#                 epochs_ls=EPOCHS,\n",
    "#                 tcn_params=TCN_PARAMS,\n",
    "#                 lstm_params=LSTM_PARAMS,\n",
    "#                 gpu_number=None,\n",
    "#                 metrics_ls=METRICS,\n",
    "#                 buffer_size=1000,\n",
    "#                 seed=1,\n",
    "#                 show_plots=False,\n",
    "#                 webhook=WEBHOOK,\n",
    "#                 validation_size=0.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "fe958679-af15-4189-ae5d-b0a3d99a1819",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "result_file_name= 'experimental_results_electricity.csv'\n",
    "metrics_ls=METRICS\n",
    "train_file_name='data/hourly_20140102_20191101_train.csv'\n",
    "test_file_name='data/hourly_20140102_20191101_test.csv'\n",
    "result_file_name='experimental_results_electricity.csv'\n",
    "forecast_horizon=FORECAST_HORIZON\n",
    "past_history_ls=PAST_HISTORY\n",
    "batch_size_ls=BATCH_SIZE\n",
    "epochs_ls=EPOCHS\n",
    "tcn_params=TCN_PARAMS\n",
    "lstm_params=None\n",
    "gpu_number=_GPU_NUMBER\n",
    "metrics_ls=METRICS\n",
    "buffer_size=1000\n",
    "seed=1\n",
    "show_plots=False\n",
    "webhook=WEBHOOK\n",
    "validation_size=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d49df411-9e46-4f01-84fd-00262088f761",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(train_file_name, test_file_name, result_file_name, forecast_horizon, past_history_ls, batch_size_ls,\n",
    "#epochs_ls, tcn_params=TCN_PARAMS, lstm_params=LSTM_PARAMS, gpu_number=None, metrics_ls=METRICS,\n",
    "#buffer_size=1000, seed=1, show_plots=False, webhook=None, validation_size=0.2)\n",
    "\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "81d83a17-c1a7-4c9b-b36f-b7fa8fa4344e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "device_name = str(gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "64e09860-408d-4ed5-86e7-b18347aa161a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CURRENT INDEX -1\n"
     ]
    }
   ],
   "source": [
    "# Write result csv header\n",
    "current_index = 0\n",
    "try:\n",
    "    with open(result_file_name, 'r') as resfile:\n",
    "        current_index = sum(1 for line in resfile) - 1\n",
    "except IOError:\n",
    "    pass\n",
    "print('CURRENT INDEX', current_index)\n",
    "if current_index == 0:\n",
    "    with open(result_file_name, 'w') as resfile:\n",
    "        resfile.write(';'.join([str(a) for a in\n",
    "                                ['MODEL', 'MODEL_DESCRIPTION', 'FORECAST_HORIZON', 'PAST_HISTORY', 'BATCH_SIZE',\n",
    "                                 'EPOCHS'] +\n",
    "                                metrics_ls + ['val_' + m for m in metrics_ls] + ['loss', 'val_loss',\n",
    "                                                                                 'Execution_time',\n",
    "                                                                                 'Device']]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26808251-c25f-4179-9b3b-7185e938cb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read train file\n",
    "with open(train_file_name, 'r') as datafile:\n",
    "    ts_train = datafile.readlines()[1:]  # skip the header\n",
    "    ts_train = np.asarray([np.asarray(l.rstrip().split(',')[0], dtype=np.float32) for l in ts_train])\n",
    "    ts_train = np.reshape(ts_train, (ts_train.shape[0],))\n",
    "\n",
    "# Read test data file\n",
    "with open(test_file_name, 'r') as datafile:\n",
    "    ts_test = datafile.readlines()[1:]  # skip the header\n",
    "    ts_test = np.asarray([np.asarray(l.rstrip().split(',')[0], dtype=np.float32) for l in ts_test])\n",
    "    ts_test = np.reshape(ts_test, (ts_test.shape[0],))\n",
    "\n",
    "# Train/validation split\n",
    "TRAIN_SPLIT = int(ts_train.shape[0] * (1 - validation_size))\n",
    "print(ts_train.shape, TRAIN_SPLIT)\n",
    "# Normalize training data\n",
    "norm_params = normalization.get_normalization_params(ts_train[:TRAIN_SPLIT])\n",
    "ts_train = normalization.normalize(ts_train, norm_params)\n",
    "# Normalize test data with train params\n",
    "ts_test = normalization.normalize(ts_test, norm_params)\n",
    "\n",
    "i = 0\n",
    "index_1, total_1 = 0, len(list(itertools.product(past_history_ls, batch_size_ls, epochs_ls)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "42fc603b-612b-49c5-8e79-bf71ca9cebe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                             | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(220671, 144, 1) (220671, 24) \n",
      " (24515, 144, 1) (24515, 24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                             | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 144, 1)]          0         \n",
      "                                                                 \n",
      " tcn_3 (TCN)                 (None, 128)               1477632   \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 24)                3096      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 24)                600       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1481328 (5.65 MB)\n",
      "Trainable params: 1481328 (5.65 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "1724/1724 [==============================] - ETA: 0s - loss: 10.5586INFO:tensorflow:Assets written to: best-model-bs-128-ph-144.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best-model-bs-128-ph-144.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1724/1724 [==============================] - 232s 132ms/step - loss: 10.5586 - val_loss: 0.5694\n",
      "Epoch 2/50\n",
      "1724/1724 [==============================] - 224s 130ms/step - loss: 0.4075 - val_loss: 0.9526\n",
      "Epoch 3/50\n",
      "1724/1724 [==============================] - ETA: 0s - loss: 0.2283INFO:tensorflow:Assets written to: best-model-bs-128-ph-144.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best-model-bs-128-ph-144.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1724/1724 [==============================] - 227s 132ms/step - loss: 0.2283 - val_loss: 0.2316\n",
      "Epoch 4/50\n",
      "1724/1724 [==============================] - ETA: 0s - loss: 0.1991INFO:tensorflow:Assets written to: best-model-bs-128-ph-144.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best-model-bs-128-ph-144.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1724/1724 [==============================] - 227s 131ms/step - loss: 0.1991 - val_loss: 0.1365\n",
      "Epoch 5/50\n",
      "1724/1724 [==============================] - ETA: 0s - loss: 0.1532INFO:tensorflow:Assets written to: best-model-bs-128-ph-144.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best-model-bs-128-ph-144.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1724/1724 [==============================] - 226s 131ms/step - loss: 0.1532 - val_loss: 0.1268\n",
      "Epoch 6/50\n",
      "1724/1724 [==============================] - 224s 130ms/step - loss: 0.1361 - val_loss: 0.1729\n",
      "Epoch 7/50\n",
      "1724/1724 [==============================] - 224s 130ms/step - loss: 0.1411 - val_loss: 0.1292\n",
      "Epoch 8/50\n",
      "1724/1724 [==============================] - 224s 130ms/step - loss: 0.1337 - val_loss: 0.1543\n",
      "Epoch 9/50\n",
      "1724/1724 [==============================] - ETA: 0s - loss: 0.1319INFO:tensorflow:Assets written to: best-model-bs-128-ph-144.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best-model-bs-128-ph-144.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1724/1724 [==============================] - 227s 132ms/step - loss: 0.1319 - val_loss: 0.1227\n",
      "Epoch 10/50\n",
      "1724/1724 [==============================] - 224s 130ms/step - loss: 0.1305 - val_loss: 0.1296\n",
      "Epoch 11/50\n",
      "1724/1724 [==============================] - 224s 130ms/step - loss: 0.1286 - val_loss: 0.1391\n",
      "Epoch 12/50\n",
      "1724/1724 [==============================] - ETA: 0s - loss: 0.1270INFO:tensorflow:Assets written to: best-model-bs-128-ph-144.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best-model-bs-128-ph-144.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1724/1724 [==============================] - 226s 131ms/step - loss: 0.1270 - val_loss: 0.1212\n",
      "Epoch 13/50\n",
      "1724/1724 [==============================] - 224s 130ms/step - loss: 1030300.1250 - val_loss: 429.5431\n",
      "Epoch 14/50\n",
      "1724/1724 [==============================] - 224s 130ms/step - loss: 286.6314 - val_loss: 182.9987\n",
      "Epoch 15/50\n",
      "1724/1724 [==============================] - 224s 130ms/step - loss: 132.2728 - val_loss: 77.8039\n",
      "Epoch 16/50\n",
      "1724/1724 [==============================] - 224s 130ms/step - loss: 66.1458 - val_loss: 19.5430\n",
      "Epoch 17/50\n",
      "1724/1724 [==============================] - 224s 130ms/step - loss: 40.2977 - val_loss: 26.8673\n",
      "Epoch 18/50\n",
      "1724/1724 [==============================] - 224s 130ms/step - loss: 29.7450 - val_loss: 12.7749\n",
      "Epoch 19/50\n",
      "1724/1724 [==============================] - 224s 130ms/step - loss: 22.5325 - val_loss: 4.5256\n",
      "Epoch 20/50\n",
      "1724/1724 [==============================] - 224s 130ms/step - loss: 3.8370 - val_loss: 5.9027\n",
      "Epoch 21/50\n",
      "1724/1724 [==============================] - 224s 130ms/step - loss: 2.3556 - val_loss: 1.8045\n",
      "Epoch 22/50\n",
      "1724/1724 [==============================] - 224s 130ms/step - loss: 1.9382 - val_loss: 5.5324\n",
      "Epoch 23/50\n",
      "1724/1724 [==============================] - 224s 130ms/step - loss: 1.1656 - val_loss: 0.4198\n",
      "Epoch 24/50\n",
      "1724/1724 [==============================] - 224s 130ms/step - loss: 505635.3750 - val_loss: 321.7757\n",
      "Epoch 25/50\n",
      "1724/1724 [==============================] - 224s 130ms/step - loss: 197.9615 - val_loss: 171.5987\n",
      "Epoch 26/50\n",
      "1724/1724 [==============================] - 224s 130ms/step - loss: 98.4125 - val_loss: 72.4734\n",
      "Epoch 27/50\n",
      "1724/1724 [==============================] - 224s 130ms/step - loss: 53.4372 - val_loss: 216.8890\n",
      "Epoch 28/50\n",
      "1724/1724 [==============================] - 224s 130ms/step - loss: 41.6707 - val_loss: 8.0865\n",
      "Epoch 29/50\n",
      "1724/1724 [==============================] - 224s 130ms/step - loss: 8.4389 - val_loss: 5.1850\n",
      "Epoch 30/50\n",
      "1724/1724 [==============================] - 224s 130ms/step - loss: 13.5818 - val_loss: 1.8724\n",
      "Epoch 31/50\n",
      "1724/1724 [==============================] - 224s 130ms/step - loss: 1.2678 - val_loss: 1.9670\n",
      "Epoch 32/50\n",
      "1724/1724 [==============================] - 224s 130ms/step - loss: 1701222.8750 - val_loss: 145839696.0000\n",
      "Epoch 33/50\n",
      "1724/1724 [==============================] - 224s 130ms/step - loss: 2430956.0000 - val_loss: 2542.9812\n",
      "Epoch 34/50\n",
      "1724/1724 [==============================] - 224s 130ms/step - loss: 2657.7598 - val_loss: 4314.4307\n",
      "Epoch 35/50\n",
      "1724/1724 [==============================] - 224s 130ms/step - loss: 1221.5161 - val_loss: 883.5305\n",
      "Epoch 36/50\n",
      "1724/1724 [==============================] - 224s 130ms/step - loss: 524.1242 - val_loss: 244.9671\n",
      "Epoch 37/50\n",
      "1724/1724 [==============================] - 224s 130ms/step - loss: 255.8728 - val_loss: 643.6802\n",
      "Epoch 38/50\n",
      "1724/1724 [==============================] - 224s 130ms/step - loss: 187.2463 - val_loss: 30.9393\n",
      "Epoch 39/50\n",
      "1724/1724 [==============================] - 224s 130ms/step - loss: 25.6523 - val_loss: 23.6755\n",
      "Epoch 40/50\n",
      "1724/1724 [==============================] - 224s 130ms/step - loss: 35.2595 - val_loss: 9.9601\n",
      "Epoch 41/50\n",
      "1724/1724 [==============================] - 224s 130ms/step - loss: 1808049.3750 - val_loss: 8606.5850\n",
      "Epoch 42/50\n",
      "1724/1724 [==============================] - 224s 130ms/step - loss: 2303.1230 - val_loss: 1486.6097\n",
      "Epoch 43/50\n",
      "1724/1724 [==============================] - 224s 130ms/step - loss: 833.2855 - val_loss: 1000.2147\n",
      "Epoch 44/50\n",
      "1724/1724 [==============================] - 224s 130ms/step - loss: 448.8559 - val_loss: 293.8879\n",
      "Epoch 45/50\n",
      "1724/1724 [==============================] - 224s 130ms/step - loss: 226.7676 - val_loss: 612.5824\n",
      "Epoch 46/50\n",
      "1724/1724 [==============================] - 227s 131ms/step - loss: 111.3213 - val_loss: 178.2980\n",
      "Epoch 47/50\n",
      "1724/1724 [==============================] - 224s 130ms/step - loss: 49.1936 - val_loss: 71.4301\n",
      "Epoch 48/50\n",
      "1724/1724 [==============================] - 224s 130ms/step - loss: 4762390.5000 - val_loss: 4379.6333\n",
      "Epoch 49/50\n",
      "1724/1724 [==============================] - 224s 130ms/step - loss: 2761.1865 - val_loss: 1981.9165\n",
      "Epoch 50/50\n",
      "1724/1724 [==============================] - 224s 130ms/step - loss: 1383.0898 - val_loss: 930.3593\n",
      "767/767 [==============================] - 9s 11ms/step\n",
      "Validation metrics {'mse': 35968392000000.0, 'rmse': 5997365.5, 'nrmse': 285.0459, 'mae': 4300427.0, 'wape': 150.36496, 'mpe': -14.498185, 'mape': 154.47717, 'mdape': 102.8626, 'smape': 194.44632530212402, 'smdape': 1.991895, 'mase': 19612.125, 'rmspe': 218.29868, 'rmsse': 27351.023, 'mre': 154.47717, 'rae': 1158.1300435640853, 'mrae': 66729368000000.0, 'std_ae': 36997212.73061761, 'std_ape': 1350.5542891651476}\n",
      "478/478 [==============================] - 17s 34ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|████████████████████████████████████████████████████████████████| 1/1 [3:07:31<00:00, 11251.19s/it]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████| 1/1 [3:07:34<00:00, 11254.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test scores {'mse': 36695280000000.0, 'rmse': 6057663.0, 'nrmse': 272.708, 'mae': 4361305.0, 'wape': 152.81757, 'mpe': -15.156453, 'mape': 158.11127, 'mdape': 104.756676, 'smape': 194.63486671447754, 'smdape': 1.9924759, 'mase': 18752.72, 'rmspe': 224.04692, 'rmsse': 26046.715, 'mre': 158.11127, 'rae': 1141.2710764785456, 'mrae': 74800190000000.0, 'std_ae': 37450286.83247433, 'std_ape': 1385.5566315173862}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for past_history, batch_size, epochs in tqdm(list(itertools.product(past_history_ls, batch_size_ls, epochs_ls))):\n",
    "    index_1 += 1\n",
    "    # Get x and y for training and validation\n",
    "    x_train, y_train = data_generation.univariate_data(ts_train, 0, TRAIN_SPLIT, past_history, forecast_horizon)\n",
    "    x_val, y_val = data_generation.univariate_data(ts_train, TRAIN_SPLIT - past_history, ts_train.shape[0],\n",
    "                                                   past_history, forecast_horizon)\n",
    "    print(x_train.shape, y_train.shape, '\\n', x_val.shape, y_val.shape)\n",
    "    # Get x and y for test data\n",
    "    x_test, y_test = data_generation.univariate_data(ts_test, 0, ts_test.shape[0], past_history, forecast_horizon)\n",
    "\n",
    "    # Convert numpy data to tensorflow dataset\n",
    "    train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train)).cache().shuffle(buffer_size).batch(\n",
    "        batch_size).repeat()\n",
    "    val_data = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(\n",
    "        batch_size).repeat() if validation_size > 0 else None\n",
    "    test_data = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
    "    \n",
    "    save_best = tf.keras.callbacks.ModelCheckpoint(filepath=f'best-model-bs-{batch_size}-ph-{past_history}-nb_stacks={nb_s}.ckpt',save_best_only=True)\n",
    "    \n",
    "    # Create models\n",
    "    model_list = {}\n",
    "    model_description_list = {}\n",
    "    if tcn_params is not None:\n",
    "        model_list = {'TCN_{}'.format(j): (tcn, [x_train.shape, forecast_horizon, 'adam', 'mae', *params]) for\n",
    "                      j, params in\n",
    "                      enumerate(itertools.product(*tcn_params.values())) if\n",
    "                      params[1] * params[2] * params[3][-1] == past_history}\n",
    "        model_description_list = {'TCN_{}'.format(j): str(dict(zip(tcn_params.keys(), params))) for j, params in\n",
    "                                  enumerate(itertools.product(*tcn_params.values())) if\n",
    "                                  params[1] * params[2] * params[3][-1] == past_history}\n",
    "    if lstm_params is not None:\n",
    "        model_list = {**model_list,\n",
    "                      **{'LSTM_{}'.format(j): (lstm, [x_train.shape, forecast_horizon, 'adam', 'mae', *params]) for\n",
    "                         j, params in\n",
    "                         enumerate(itertools.product(*lstm_params.values()))}}\n",
    "        model_description_list = {**model_description_list,\n",
    "                                  **{'LSTM_{}'.format(j): str(dict(zip(lstm_params.keys(), params))) for j, params\n",
    "                                     in\n",
    "                                     enumerate(itertools.product(*lstm_params.values()))}}\n",
    "\n",
    "    steps_per_epoch = int(np.ceil(x_train.shape[0] / batch_size))\n",
    "    validation_steps = steps_per_epoch if val_data else None\n",
    "\n",
    "    index_2, total_2 = 0, len(model_list.keys())\n",
    "    for model_name, (model_function, params) in tqdm(model_list.items(), position=1):\n",
    "        index_2 += 1\n",
    "        i += 1\n",
    "        if i <= current_index:\n",
    "            continue\n",
    "        start = time.time()\n",
    "        model = model_function(*params)\n",
    "        print(model.summary())\n",
    "\n",
    "        # Train the model\n",
    "        history = model.fit(train_data, epochs=epochs,\n",
    "                            steps_per_epoch=steps_per_epoch,\n",
    "                            validation_data=val_data, \n",
    "                            validation_steps=validation_steps,\n",
    "                            callbacks=[save_best]\n",
    "                           )\n",
    "        \n",
    "        # Plot training and evaluation loss evolution\n",
    "        if show_plots:\n",
    "            auxiliary_plots.plot_training_history(history, ['loss'])\n",
    "\n",
    "        # Get validation results\n",
    "        val_metrics = {}\n",
    "        if validation_size > 0:\n",
    "            val_forecast = model.predict(x_val)\n",
    "            val_forecast = normalization.denormalize(val_forecast, norm_params)\n",
    "            y_val_denormalized = normalization.denormalize(y_val, norm_params)\n",
    "\n",
    "            val_metrics = metrics.evaluate(y_val_denormalized, val_forecast, metrics_ls)\n",
    "            print('Validation metrics', val_metrics)\n",
    "\n",
    "        # TEST\n",
    "        # Predict with test data and get results\n",
    "        test_forecast = model.predict(test_data)\n",
    "\n",
    "        test_forecast = normalization.denormalize(test_forecast, norm_params)\n",
    "        y_test_denormalized = normalization.denormalize(y_test, norm_params)\n",
    "        x_test_denormalized = normalization.denormalize(x_test, norm_params)\n",
    "\n",
    "        test_metrics = metrics.evaluate(y_test_denormalized, test_forecast, metrics_ls)\n",
    "        print('Test scores', test_metrics)\n",
    "\n",
    "        # Plot some test predictions\n",
    "        if show_plots:\n",
    "            auxiliary_plots.plot_ts_forecasts(x_test_denormalized, y_test_denormalized, test_forecast)\n",
    "\n",
    "        # Save results\n",
    "        val_metrics = {'val_' + k: val_metrics[k] for k in val_metrics}\n",
    "        model_metric = {'MODEL': model_name,\n",
    "                        'MODEL_DESCRIPTION': model_description_list[model_name],\n",
    "                        'FORECAST_HORIZON': forecast_horizon,\n",
    "                        'PAST_HISTORY': past_history,\n",
    "                        'BATCH_SIZE': batch_size,\n",
    "                        'EPOCHS': epochs,\n",
    "                        **test_metrics,\n",
    "                        **val_metrics,\n",
    "                        **history.history,\n",
    "                        'Execution_time': time.time() - start,\n",
    "                        'Device': device_name\n",
    "                        }\n",
    "\n",
    "        #notify_slack('Progress: {0}/{1} ({2}/{3}) \\nMetrics:{4}'.format(index_1, total_1, index_2, total_2,\n",
    "        #                                                                str({'Model': model_name,\n",
    "        #                                                                     'WAPE': str(test_metrics['wape']),\n",
    "        #                                                                     'Execution_time': \"{0:.2f}  seconds\".format(\n",
    "        #                                                                         time.time() - start)\n",
    "        #                                                                     })), webhook=webhook)\n",
    "\n",
    "        with open(result_file_name, 'a') as resfile:\n",
    "            resfile.write(';'.join([str(a) for a in model_metric.values()]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688257fd-fd14-4575-a8d8-7bc8b3687b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406c03da-09dc-4899-a0a6-e335c8648bb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec5b7b9-b20c-4810-912f-40a06bcb7a80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3a8478-6146-46c7-8b02-ac310a11a047",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
